{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1092c8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64933a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test.csv', index_col=0)\n",
    "train_df = pd.read_csv('train.csv', index_col=0)\n",
    "code_d = pd.read_csv('D_code_new.csv')\n",
    "code_h = pd.read_csv('H_code_new.csv')\n",
    "code_l = pd.read_csv('L_code.csv')\n",
    "submission_df = pd.read_csv('sample_submission.csv')\n",
    "# code_d.drop(labels='Unnamed: 5', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f6d3415",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['d_l_match_yn', 'd_m_match_yn', 'd_s_match_yn', 'h_l_match_yn', 'h_m_match_yn', 'h_s_match_yn'] :\n",
    "    train_df[i] = train_df[i].astype('int')\n",
    "    test_df[i] = test_df[i].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d037b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.50026\n",
       "1    0.49974\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target 비율\n",
    "train_df.target.value_counts(sort=False)/len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d560596f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# null 수\n",
    "sum(train_df[train_df.isna()].count()) # 0\n",
    "sum(test_df[test_df.isna()].count())   # 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fbe73c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_person_df = train_df.iloc[:,7:19]\n",
    "train_content_df = train_df.iloc[:,19:30]\n",
    "test_person_df = test_df.iloc[:,7:19]\n",
    "test_content_df = test_df.iloc[:,19:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ab0301",
   "metadata": {},
   "source": [
    "# random forest baseline 0.62704"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867951ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# train_x = train_df.iloc[:,:30]\n",
    "# train_y = train_df.iloc[:,[-1]]\n",
    "# test_x = test_df.iloc[:,:30]\n",
    "# test_y = test_df.iloc[:,[-1]]\n",
    "\n",
    "# rf = RandomForestClassifier(max_depth=20)\n",
    "# rf.fit(train_x, train_y)\n",
    "# print(rf.score(train_x, train_y))  # 0.8726947451046019\n",
    "# pred_y = rf.predict(test_x)\n",
    "\n",
    "# submission_df['target'] = pred_y\n",
    "# # submission_df.to_csv('sub1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573f050e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "128510b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_d.columns = ['d_code', 'd_section_code', 'd_small_code', 'd_midium_code', 'd_large_code']\n",
    "code_l.columns = ['l_code', 'l_section_code', 'l_small_code', 'l_midium_code', 'l_large_code']\n",
    "code_h.columns = ['h_code', 'h_midium_code', 'h_large_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fbc6cb7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c07d113c774c4bbaab5cddc43fbed1d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TARGET = test_person_df\n",
    "\n",
    "d = ['person_prefer_d_1', 'person_prefer_d_2', 'person_prefer_d_3']\n",
    "h = ['person_prefer_h_1', 'person_prefer_h_2', 'person_prefer_h_3']\n",
    "\n",
    "person_res_d = pd.DataFrame()\n",
    "person_res_h = pd.DataFrame()\n",
    "\n",
    "for idx in tqdm(range(len(TARGET))):\n",
    "    tmp_d = pd.DataFrame()\n",
    "    tmp_h = pd.DataFrame()\n",
    "    for col in d :  # ['person_prefer_d_1', 'person_prefer_d_2', 'person_prefer_d_3']  #  ['contents_attribute_d']\n",
    "        tmp_code_num = TARGET[col][idx]\n",
    "        t = code_d[code_d['d_code']==tmp_code_num]\n",
    "        t.drop('d_code', axis=1, inplace=True)\n",
    "        t.index = [idx]\n",
    "        t = t.add_prefix(col)\n",
    "        tmp_d = pd.concat([tmp_d, t], axis=1)\n",
    "    person_res_d = person_res_d.append(tmp_d, ignore_index=True)\n",
    "    \n",
    "    for col in h :  # ['person_prefer_d_1', 'person_prefer_d_2', 'person_prefer_d_3']  #  ['contents_attribute_d']\n",
    "        tmp_code_num = TARGET[col][idx]\n",
    "        t = code_h[code_h['h_code']==tmp_code_num]\n",
    "        t.drop('h_code', axis=1, inplace=True)\n",
    "        t.index = [idx]\n",
    "        t = t.add_prefix(col)\n",
    "        tmp_h = pd.concat([tmp_h, t], axis=1)\n",
    "    person_res_h = person_res_h.append(tmp_h, ignore_index=True)\n",
    "    \n",
    "    \n",
    "# res.to_csv('person_d_code_detail.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d85a05b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecd4c85c4476400b897f3effda83c089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# content\n",
    "\n",
    "d = ['contents_attribute_d']\n",
    "h = ['contents_attribute_h']\n",
    "l = ['contents_attribute_l']\n",
    "\n",
    "TARGET = test_content_df\n",
    "\n",
    "content_res_d = pd.DataFrame()\n",
    "content_res_h = pd.DataFrame()\n",
    "content_res_l = pd.DataFrame()\n",
    "\n",
    "for idx in tqdm(range(len(TARGET))):\n",
    "    tmp_d = pd.DataFrame()\n",
    "    tmp_h = pd.DataFrame()\n",
    "    tmp_l = pd.DataFrame()\n",
    "    for col in d :  # ['contents_attribute_d']\n",
    "        tmp_code_num = TARGET[col][idx]\n",
    "        t = code_d[code_d['d_code']==tmp_code_num]\n",
    "        t.drop('d_code', axis=1, inplace=True)\n",
    "        t.index = [idx]\n",
    "        t = t.add_prefix(col)\n",
    "        tmp_d = pd.concat([tmp_d, t], axis=1)\n",
    "    content_res_d = content_res_d.append(tmp_d, ignore_index=True)\n",
    "    \n",
    "    for col in h :\n",
    "        tmp_code_num = TARGET[col][idx]\n",
    "        t = code_h[code_h['h_code']==tmp_code_num]\n",
    "        t.drop('h_code', axis=1, inplace=True)\n",
    "        t.index = [idx]\n",
    "        t = t.add_prefix(col)\n",
    "        tmp_h = pd.concat([tmp_h, t], axis=1)\n",
    "    content_res_h = content_res_h.append(tmp_h, ignore_index=True)\n",
    "    \n",
    "    for col in l : \n",
    "        tmp_code_num = TARGET[col][idx]\n",
    "        t = code_l[code_l['l_code']==tmp_code_num]\n",
    "        t.drop('l_code', axis=1, inplace=True)\n",
    "        t.index = [idx]\n",
    "        t = t.add_prefix(col)\n",
    "        tmp_l = pd.concat([tmp_l, t], axis=1)\n",
    "    content_res_l = content_res_l.append(tmp_l, ignore_index=True)\n",
    "    \n",
    "# content_res_l.to_csv('content_l_code_detail.csv', index=False)\n",
    "# content_res_h.to_csv('content_h_code_detail.csv', index=False)\n",
    "# content_res_d.to_csv('content_d_code_detail.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93400c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_x = pd.concat([train_df.iloc[:,[0,1,2,3,4,5,30,31]], train_person_df, person_res_d, person_res_h, train_content_df, content_res_d, content_res_h, content_res_l], axis=1)\n",
    "new_train_y = train_df.iloc[:,[-1]]\n",
    "\n",
    "new_test_x = pd.concat([test_df.iloc[:,[0,1,2,3,4,5,30,31]], test_person_df, person_res_d, person_res_h, test_content_df, content_res_d, content_res_h, content_res_l], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d2554a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3cb2e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "cda67261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "new_train_x, new_val_x, new_train_y, new_val_y = train_test_split(new_train_x, new_train_y, test_size=0.3 ,random_state=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "af86e075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "756d4a3a45e0498a8166e5d47c7a7bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth : 20\n",
      "  score   : 0.6143439901337634\n",
      "\n",
      "max_depth : 25\n",
      "  score   : 0.6142301489422256\n",
      "\n",
      "max_depth : 30\n",
      "  score   : 0.6127597002181956\n",
      "\n",
      "max_depth : 35\n",
      "  score   : 0.6117256427283939\n",
      "\n",
      "max_depth : 40\n",
      "  score   : 0.6111090029408974\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "for i in tqdm([20,25,30,35,40]) :\n",
    "    rf = RandomForestClassifier(max_depth=i, random_state=777)\n",
    "    rf.fit(new_train_x, new_train_y)\n",
    "    score = rf.score(new_val_x, new_val_y)  # 0.8610222910204383\n",
    "    \n",
    "    print(f'max_depth : {i}')\n",
    "    print(f'  score   : {score}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "24126c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier :  0.6014135281282611\n",
      "AdaBoostClassifier :  0.5931315814438858\n",
      "ExtraTreesClassifier :  0.604961578597856\n",
      "BaggingClassifier :  0.5761977042026373\n",
      "[01:24:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier :  0.5920216298263922\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "model = GradientBoostingClassifier(random_state=777)\n",
    "model.fit(new_train_x, new_train_y)\n",
    "score = model.score(new_val_x, new_val_y)\n",
    "print('GradientBoostingClassifier : ',score)\n",
    "\n",
    "model = AdaBoostClassifier(random_state=777)\n",
    "model.fit(new_train_x, new_train_y)\n",
    "score = model.score(new_val_x, new_val_y)\n",
    "print('AdaBoostClassifier : ',score)\n",
    "\n",
    "model = ExtraTreesClassifier(random_state=777)\n",
    "model.fit(new_train_x, new_train_y)\n",
    "score = model.score(new_val_x, new_val_y)\n",
    "print('ExtraTreesClassifier : ',score)\n",
    "\n",
    "model = BaggingClassifier(random_state=777)\n",
    "model.fit(new_train_x, new_train_y)\n",
    "score = model.score(new_val_x, new_val_y)\n",
    "print('BaggingClassifier : ',score)\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "model = XGBClassifier(n_estimators=100, learning_rate=0.01, max_depth=30)\n",
    "model.fit(new_train_x, new_train_y)\n",
    "score = model.score(new_val_x, new_val_y)\n",
    "print('XGBClassifier : ',score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4ea84b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8ba886",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "xgb_param_grid = {\n",
    "    'n_estimators' : [5],  # 200,400,600\n",
    "    'learning_rate' : [0.01, 0.05, 0.1, 0.15, 0.2],  #  \n",
    "    'max_depth' : [10, 20]  # \n",
    "}\n",
    "\n",
    "xgb_grid = GridSearchCV(xgb, param_grid=xgb_param_grid, scoring='accuracy', n_jobs=-1, cv=3 ,verbose=10)\n",
    "xgb_grid.fit(new_train_x, new_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f588fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('최고 평균 정확도 : {0: .4f}'.format(xgb_grid.best_score_))\n",
    "print('최고의 파라미터 : ', xgb_grid.best_params_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6090636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_result_df = pd.DataFrame(xgb_grid.cv_results_)\n",
    "cv_result_df.sort_values(by=['rank_test_score'], inplace=True)\n",
    "cv_result_df[['params', 'mean_test_score', 'rank_test_score']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95db928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "460f7773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7685a94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7a95fbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'learning_rate' : 0.05,\n",
    "        'max_depth' : 32,\n",
    "    'objective' : 'binary',\n",
    "    'metric' : 'binary_logloss',\n",
    "    'is_training_metric' : True,\n",
    "    'num_leave' : 144,\n",
    "    'feature_fraction' : 0.9,\n",
    "    'bagging_fraction' : 0.7,\n",
    "    'bagging_freq':5,\n",
    "    'seed':2020\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "066aa98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: num_leave\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Unknown parameter: num_leave\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 122741, number of negative: 123214\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011318 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5798\n",
      "[LightGBM] [Info] Number of data points in the train set: 245955, number of used features: 57\n",
      "[LightGBM] [Warning] Unknown parameter: num_leave\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499038 -> initscore=-0.003846\n",
      "[LightGBM] [Info] Start training from score -0.003846\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.655125\n",
      "[200]\tvalid_0's binary_logloss: 0.650167\n",
      "[300]\tvalid_0's binary_logloss: 0.648249\n",
      "[400]\tvalid_0's binary_logloss: 0.64672\n",
      "[500]\tvalid_0's binary_logloss: 0.64588\n",
      "[600]\tvalid_0's binary_logloss: 0.645307\n",
      "[700]\tvalid_0's binary_logloss: 0.64477\n",
      "[800]\tvalid_0's binary_logloss: 0.644197\n",
      "[900]\tvalid_0's binary_logloss: 0.643884\n",
      "[1000]\tvalid_0's binary_logloss: 0.64361\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[995]\tvalid_0's binary_logloss: 0.64359\n"
     ]
    }
   ],
   "source": [
    "train_dataset = lgb.Dataset(new_train_x, label=new_train_y)\n",
    "val_dataset = lgb.Dataset(new_val_x, label=new_val_y)\n",
    "\n",
    "model = lgb.train(params, train_dataset, 1000, val_dataset, verbose_eval=100, early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06d8b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_df.iloc[:,:30]\n",
    "train_y = train_df.iloc[:,[-1]]\n",
    "\n",
    "default_train_x, default_val_x, default_train_y, default_val_y = train_test_split(train_x, train_y, test_size=0.3, random_state=777)\n",
    "\n",
    "default_train_dataset = lgb.Dataset(default_train_x, label=default_train_y)\n",
    "default_val_dataset = lgb.Dataset(default_val_x, label=default_val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e522b248",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.train(params, default_train_dataset, 1000, default_val_dataset, verbose_eval=100, early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0fc78b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6239635708187079"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y = model.predict(new_val_x)\n",
    "pred_y = [1 if i>0.5 else 0 for i in pred_y]\n",
    "accuracy_score(new_val_y, pred_y)  \n",
    "# 0.6171755674498294\n",
    "# 0.6222225173654922\n",
    "# 0.6269440718260662 -> 0.64806  // 2021-12-23 14:54\n",
    "# 0.6292749658002736 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "47a0cb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = model.predict(new_test_x)\n",
    "pred_y = [1 if i>0.5 else 0 for i in pred_y]\n",
    "submission_df['target'] = pred_y\n",
    "submission_df.to_csv('sub8.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "baf02545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b16061f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c97f15c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 load\n",
    "train_x_df = pd.read_csv('new_train_x.csv')\n",
    "train_y_df = pd.read_csv('new_train_y.csv')\n",
    "test_x_df = pd.read_csv('new_test_x.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f547d6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_df.drop(['person_rn','contents_rn','person_prefer_g', 'person_prefer_f'], axis=1, inplace=True)\n",
    "test_x_df.drop(['person_rn','contents_rn','person_prefer_g', 'person_prefer_f'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b3e200",
   "metadata": {},
   "source": [
    "# 효과 X\n",
    "train_match_count = []\n",
    "test_match_count = []\n",
    "for i in tqdm(range(len(train_df))) :\n",
    "    train_match_count.append(train_x_df.iloc[i,:6].sum())\n",
    "for i in tqdm(range(len(test_x_df))) :\n",
    "    test_match_count.append(test_x_df.iloc[i,:6].sum())\n",
    "    \n",
    "    \n",
    "train_x_df['match_count'] = train_match_count\n",
    "test_x_df['match_count'] = test_match_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9a06152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86862a10701641c69720d01e36baad08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# for col in tqdm(['person_attribute_a_1', 'person_prefer_c', 'contents_attribute_i', 'contents_attribute_a',\n",
    "#                 'contents_attribute_j_1', 'contents_attribute_j', 'contents_attribute_c', 'contents_attribute_k',\n",
    "#                 'contents_attribute_m']) :\n",
    "    \n",
    "#     train_x_df = pd.concat([train_x_df, pd.get_dummies(train_x_df[col])], axis=1)\n",
    "#     train_x_df.drop([col], axis=1, inplace=True)\n",
    "#     test_x_df = pd.concat([test_x_df, pd.get_dummies(test_x_df[col])], axis=1)\n",
    "#     test_x_df.drop([col], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9fa79c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_df['person_attribute_a'] = train_df['person_attribute_a']\n",
    "test_x_df['person_attribute_a'] = test_df['person_attribute_a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5fe6af74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_person_attribute_a_1_list = list(train_x_df[train_x_df['person_attribute_a']==1].index)\n",
    "train_person_attribute_a_2_list = list(train_x_df[train_x_df['person_attribute_a']==2].index)\n",
    "\n",
    "test_person_attribute_a_1_list = list(test_x_df[test_x_df['person_attribute_a']==1].index)\n",
    "test_person_attribute_a_2_list = list(test_x_df[test_x_df['person_attribute_a']==2].index)\n",
    "\n",
    "train_1_x = train_x_df.iloc[train_person_attribute_a_1_list,:]\n",
    "train_1_y = train_y_df.iloc[train_person_attribute_a_1_list,:]\n",
    "\n",
    "train_2_x = train_x_df.iloc[train_person_attribute_a_2_list,:]\n",
    "train_2_y = train_y_df.iloc[train_person_attribute_a_2_list,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9a4b0270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "68a439c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # person과 content를 해싱해 새로운 변수를 만들고 학습 --> 0.58 정확도\n",
    "\n",
    "# from category_encoders.hashing import HashingEncoder\n",
    "\n",
    "# person_col = ['person_attribute_a','person_attribute_a_1',\n",
    "#        'person_attribute_b', 'person_prefer_c', 'person_prefer_d_1',\n",
    "#        'person_prefer_d_2', 'person_prefer_d_3', 'person_prefer_e',\n",
    "#        'person_prefer_h_1', 'person_prefer_h_2', 'person_prefer_h_3',\n",
    "#        'person_prefer_d_1d_section_code', 'person_prefer_d_1d_small_code',\n",
    "#        'person_prefer_d_1d_midium_code', 'person_prefer_d_1d_large_code',\n",
    "#        'person_prefer_d_2d_section_code', 'person_prefer_d_2d_small_code',\n",
    "#        'person_prefer_d_2d_midium_code', 'person_prefer_d_2d_large_code',\n",
    "#        'person_prefer_d_3d_section_code', 'person_prefer_d_3d_small_code',\n",
    "#        'person_prefer_d_3d_midium_code', 'person_prefer_d_3d_large_code',\n",
    "#        'person_prefer_h_1h_midium_code', 'person_prefer_h_1h_large_code',\n",
    "#        'person_prefer_h_2h_midium_code', 'person_prefer_h_2h_large_code',\n",
    "#        'person_prefer_h_3h_midium_code', 'person_prefer_h_3h_large_code']\n",
    "# contents_col = ['contents_attribute_i', 'contents_attribute_a',\n",
    "#        'contents_attribute_j_1', 'contents_attribute_j',\n",
    "#        'contents_attribute_c', 'contents_attribute_k', 'contents_attribute_l',\n",
    "#        'contents_attribute_d', 'contents_attribute_m', 'contents_attribute_e',\n",
    "#        'contents_attribute_h', 'contents_attribute_dd_section_code',\n",
    "#        'contents_attribute_dd_small_code', 'contents_attribute_dd_midium_code',\n",
    "#        'contents_attribute_dd_large_code', 'contents_attribute_hh_midium_code',\n",
    "#        'contents_attribute_hh_large_code',\n",
    "#        'contents_attribute_ll_section_code',\n",
    "#        'contents_attribute_ll_small_code', 'contents_attribute_ll_midium_code',\n",
    "#        'contents_attribute_ll_large_code']\n",
    "\n",
    "# p = HashingEncoder(cols=person_col).fit_transform(train_x_df)\n",
    "# c = HashingEncoder(cols=contents_col).fit_transform(p)\n",
    "# c\n",
    "\n",
    "# c.columns = list(range(len(c.columns)))\n",
    "# c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "136650df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_df.columns = list(range(len(train_x_df.columns)))\n",
    "test_x_df.columns = list(range(len(test_x_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8294003b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "new_train_x, new_val_x, new_train_y, new_val_y = train_test_split(train_x_df, train_y_df, test_size=0.3 ,random_state=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea22db27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: num_leave\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Unknown parameter: num_leave\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 175481, number of negative: 175884\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.161941 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5289\n",
      "[LightGBM] [Info] Number of data points in the train set: 351365, number of used features: 56\n",
      "[LightGBM] [Warning] Unknown parameter: num_leave\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499427 -> initscore=-0.002294\n",
      "[LightGBM] [Info] Start training from score -0.002294\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's binary_logloss: 0.648651\n",
      "[400]\tvalid_0's binary_logloss: 0.644636\n",
      "[600]\tvalid_0's binary_logloss: 0.642869\n",
      "[800]\tvalid_0's binary_logloss: 0.641652\n",
      "[1000]\tvalid_0's binary_logloss: 0.640657\n",
      "[1200]\tvalid_0's binary_logloss: 0.639905\n",
      "[1400]\tvalid_0's binary_logloss: 0.639212\n",
      "[1600]\tvalid_0's binary_logloss: 0.63886\n",
      "[1800]\tvalid_0's binary_logloss: 0.63842\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "params = {\n",
    "    'learning_rate' : 0.05,\n",
    "        'max_depth' : 1024,\n",
    "    'objective' : 'binary',\n",
    "    'metric' : 'binary_logloss',\n",
    "    'is_training_metric' : True,\n",
    "    'num_leave' : 144,\n",
    "    'feature_fraction' : 0.9,\n",
    "    'bagging_fraction' : 0.7,\n",
    "    'bagging_freq':5,\n",
    "    'seed':2020\n",
    "}\n",
    "\n",
    "train_dataset = lgb.Dataset(new_train_x, label=new_train_y)\n",
    "val_dataset = lgb.Dataset(new_val_x, label=new_val_y)\n",
    "\n",
    "model = lgb.train(params, train_dataset, 2000, val_dataset, verbose_eval=200, early_stopping_rounds=100)\n",
    "\n",
    "pred_y = model.predict(new_val_x)\n",
    "pred_y = [1 if i>0.5 else 0 for i in pred_y]\n",
    "accuracy_score(new_val_y, pred_y)  \n",
    "# 0.6171755674498294\n",
    "# 0.6222225173654922\n",
    "# 0.6269440718260662 -> 0.64806  // 2021-12-23 14:54\n",
    "# 0.6292749658002736 \n",
    "\n",
    "# 0.6118177770033494\n",
    "\n",
    "# 0.6306296734092147  -> 2021-12-28 15:25:05\t0.644898965\n",
    "\n",
    "# 0.6308421765635583"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf70324f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = model.predict(test_x_df)\n",
    "pred_y = [1 if i>0.5 else 0 for i in pred_y]\n",
    "submission_df['target'] = pred_y\n",
    "submission_df.to_csv('sub10.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7555adfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DNN \n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(128, activation='relu', input_shape=(87,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(new_train_x,\n",
    "                    new_train_y,\n",
    "                    epochs=1000,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(new_val_x, new_val_y))\n",
    "results = model.evaluate(new_val_x, new_val_y)\n",
    "results\n",
    "\n",
    "# Epoch 725/1000\n",
    "# 687/687 [==============================] - 2s 2ms/step - loss: 0.6474 - accuracy: 0.6095 - val_loss: 0.6670 - val_accuracy: 0.5998\n",
    "\n",
    "pred_y = model.predict(new_val_x)\n",
    "pred_y = [1 if i>0.5 else 0 for i in pred_y]\n",
    "\n",
    "accuracy_score(pred_y, new_val_y)  # 0.5975323071201838"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
